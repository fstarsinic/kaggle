{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Did Kobe Bryant sink the shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# initiate our session and read the main CSV file, then we print the dataframe schema\n",
    "\n",
    "spark = SparkSession.builder.appName('imbalanced_binary_classification').getOrCreate()\n",
    "df = spark.read.csv('data.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLlib uses only \"features\" and \"labels\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- action_type: string (nullable = true)\n",
      " |-- combined_shot_type: string (nullable = true)\n",
      " |-- game_event_id: integer (nullable = true)\n",
      " |-- game_id: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- loc_x: integer (nullable = true)\n",
      " |-- loc_y: integer (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- minutes_remaining: integer (nullable = true)\n",
      " |-- period: integer (nullable = true)\n",
      " |-- playoffs: integer (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- seconds_remaining: integer (nullable = true)\n",
      " |-- shot_distance: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- shot_type: string (nullable = true)\n",
      " |-- shot_zone_area: string (nullable = true)\n",
      " |-- shot_zone_basic: string (nullable = true)\n",
      " |-- shot_zone_range: string (nullable = true)\n",
      " |-- team_id: integer (nullable = true)\n",
      " |-- team_name: string (nullable = true)\n",
      " |-- game_date: timestamp (nullable = true)\n",
      " |-- matchup: string (nullable = true)\n",
      " |-- opponent: string (nullable = true)\n",
      " |-- shot_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rename the shot_made_flag to \"label\", as required\n",
    "df = df.withColumnRenamed('shot_made_flag', 'label')\n",
    "new_df = df\n",
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of columns we do not want to train with\n",
    "drop_col = ['game_id','game_event_id','team_id','team_name','game_date','combined_shot_type','lat','lon','shot_zone_basic','shot_id']\n",
    "new_df = new_df.select([column for column in new_df.columns if column not in drop_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_type</th>\n",
       "      <th>loc_x</th>\n",
       "      <th>loc_y</th>\n",
       "      <th>minutes_remaining</th>\n",
       "      <th>period</th>\n",
       "      <th>playoffs</th>\n",
       "      <th>season</th>\n",
       "      <th>seconds_remaining</th>\n",
       "      <th>shot_distance</th>\n",
       "      <th>label</th>\n",
       "      <th>shot_type</th>\n",
       "      <th>shot_zone_area</th>\n",
       "      <th>shot_zone_range</th>\n",
       "      <th>matchup</th>\n",
       "      <th>opponent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jump Shot</td>\n",
       "      <td>167</td>\n",
       "      <td>72</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>None</td>\n",
       "      <td>2PT Field Goal</td>\n",
       "      <td>Right Side(R)</td>\n",
       "      <td>16-24 ft.</td>\n",
       "      <td>LAL @ POR</td>\n",
       "      <td>POR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  action_type  loc_x  loc_y  minutes_remaining  period  playoffs   season  \\\n",
       "0   Jump Shot    167     72                 10       1         0  2000-01   \n",
       "\n",
       "   seconds_remaining  shot_distance label       shot_type shot_zone_area  \\\n",
       "0                 27             18  None  2PT Field Goal  Right Side(R)   \n",
       "\n",
       "  shot_zone_range    matchup opponent  \n",
       "0       16-24 ft.  LAL @ POR      POR  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out the possible label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>11465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  count\n",
       "0    NaN   5000\n",
       "1    1.0  11465\n",
       "2    0.0  14232"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.groupby('label').count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The null value labels, are the data we need to make predictions on, for the competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = df.filter('label is null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_type</th>\n",
       "      <th>combined_shot_type</th>\n",
       "      <th>game_event_id</th>\n",
       "      <th>game_id</th>\n",
       "      <th>lat</th>\n",
       "      <th>loc_x</th>\n",
       "      <th>loc_y</th>\n",
       "      <th>lon</th>\n",
       "      <th>minutes_remaining</th>\n",
       "      <th>period</th>\n",
       "      <th>...</th>\n",
       "      <th>shot_type</th>\n",
       "      <th>shot_zone_area</th>\n",
       "      <th>shot_zone_basic</th>\n",
       "      <th>shot_zone_range</th>\n",
       "      <th>team_id</th>\n",
       "      <th>team_name</th>\n",
       "      <th>game_date</th>\n",
       "      <th>matchup</th>\n",
       "      <th>opponent</th>\n",
       "      <th>shot_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jump Shot</td>\n",
       "      <td>Jump Shot</td>\n",
       "      <td>10</td>\n",
       "      <td>20000012</td>\n",
       "      <td>33.9723</td>\n",
       "      <td>167</td>\n",
       "      <td>72</td>\n",
       "      <td>-118.1028</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2PT Field Goal</td>\n",
       "      <td>Right Side(R)</td>\n",
       "      <td>Mid-Range</td>\n",
       "      <td>16-24 ft.</td>\n",
       "      <td>1610612747</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>2000-10-31</td>\n",
       "      <td>LAL @ POR</td>\n",
       "      <td>POR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jump Shot</td>\n",
       "      <td>Jump Shot</td>\n",
       "      <td>254</td>\n",
       "      <td>20000012</td>\n",
       "      <td>34.0163</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>-118.2688</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2PT Field Goal</td>\n",
       "      <td>Center(C)</td>\n",
       "      <td>Restricted Area</td>\n",
       "      <td>Less Than 8 ft.</td>\n",
       "      <td>1610612747</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>2000-10-31</td>\n",
       "      <td>LAL @ POR</td>\n",
       "      <td>POR</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Driving Layup Shot</td>\n",
       "      <td>Layup</td>\n",
       "      <td>100</td>\n",
       "      <td>20000019</td>\n",
       "      <td>34.0443</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-118.2698</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2PT Field Goal</td>\n",
       "      <td>Center(C)</td>\n",
       "      <td>Restricted Area</td>\n",
       "      <td>Less Than 8 ft.</td>\n",
       "      <td>1610612747</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>2000-11-01</td>\n",
       "      <td>LAL vs. UTA</td>\n",
       "      <td>UTA</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Driving Layup Shot</td>\n",
       "      <td>Layup</td>\n",
       "      <td>249</td>\n",
       "      <td>20000019</td>\n",
       "      <td>34.0443</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-118.2698</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2PT Field Goal</td>\n",
       "      <td>Center(C)</td>\n",
       "      <td>Restricted Area</td>\n",
       "      <td>Less Than 8 ft.</td>\n",
       "      <td>1610612747</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>2000-11-01</td>\n",
       "      <td>LAL vs. UTA</td>\n",
       "      <td>UTA</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jump Shot</td>\n",
       "      <td>Jump Shot</td>\n",
       "      <td>4</td>\n",
       "      <td>20000047</td>\n",
       "      <td>33.9683</td>\n",
       "      <td>163</td>\n",
       "      <td>76</td>\n",
       "      <td>-118.1068</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2PT Field Goal</td>\n",
       "      <td>Right Side(R)</td>\n",
       "      <td>Mid-Range</td>\n",
       "      <td>16-24 ft.</td>\n",
       "      <td>1610612747</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>2000-11-04</td>\n",
       "      <td>LAL @ VAN</td>\n",
       "      <td>VAN</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          action_type combined_shot_type  game_event_id   game_id      lat  \\\n",
       "0           Jump Shot          Jump Shot             10  20000012  33.9723   \n",
       "1           Jump Shot          Jump Shot            254  20000012  34.0163   \n",
       "2  Driving Layup Shot              Layup            100  20000019  34.0443   \n",
       "3  Driving Layup Shot              Layup            249  20000019  34.0443   \n",
       "4           Jump Shot          Jump Shot              4  20000047  33.9683   \n",
       "\n",
       "   loc_x  loc_y       lon  minutes_remaining  period  ...       shot_type  \\\n",
       "0    167     72 -118.1028                 10       1  ...  2PT Field Goal   \n",
       "1      1     28 -118.2688                  8       3  ...  2PT Field Goal   \n",
       "2      0      0 -118.2698                  0       1  ...  2PT Field Goal   \n",
       "3      0      0 -118.2698                 10       3  ...  2PT Field Goal   \n",
       "4    163     76 -118.1068                 11       1  ...  2PT Field Goal   \n",
       "\n",
       "  shot_zone_area  shot_zone_basic  shot_zone_range     team_id  \\\n",
       "0  Right Side(R)        Mid-Range        16-24 ft.  1610612747   \n",
       "1      Center(C)  Restricted Area  Less Than 8 ft.  1610612747   \n",
       "2      Center(C)  Restricted Area  Less Than 8 ft.  1610612747   \n",
       "3      Center(C)  Restricted Area  Less Than 8 ft.  1610612747   \n",
       "4  Right Side(R)        Mid-Range        16-24 ft.  1610612747   \n",
       "\n",
       "            team_name  game_date      matchup opponent  shot_id  \n",
       "0  Los Angeles Lakers 2000-10-31    LAL @ POR      POR        1  \n",
       "1  Los Angeles Lakers 2000-10-31    LAL @ POR      POR        8  \n",
       "2  Los Angeles Lakers 2000-11-01  LAL vs. UTA      UTA       17  \n",
       "3  Los Angeles Lakers 2000-11-01  LAL vs. UTA      UTA       20  \n",
       "4  Los Angeles Lakers 2000-11-04    LAL @ VAN      VAN       33  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = new_df.filter(\"label=1\").count()\n",
    "zeros = new_df.filter(\"label=0\").count()\n",
    "\n",
    "total = ones + zeros\n",
    "print('all: {}'.format(total))\n",
    "print('ones: {}'.format(ones))\n",
    "print('zeros: {}'.format(zeros))\n",
    "ratio = round(zeros/total,2)\n",
    "print('ratio is: {}'.format(ratio))\n",
    "\n",
    "imbalanced = False\n",
    "if ratio > .7:\n",
    "    imbalanced = True\n",
    "    print('This is an imbalanced dataset: {}'.format(ratio))\n",
    "else:\n",
    "    imbalanced = False\n",
    "    print('This is not an imbalanced dataset: {}'.format(ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the distribution of our target variable:\n",
    "# to make it look better, we first convert our spark df to a Pandas df\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "df_pd = new_df.toPandas()\n",
    "print(len(df_pd))\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.countplot(x='label', data=df_pd, order=df_pd['label'].value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's see how everything look in Pandas\n",
    "import pandas as pd\n",
    "pd.DataFrame(new_df.take(10), columns= new_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in new_df.dtypes:\n",
    "    print('{} : {}'.format(item[0], item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the below function to find more information about the missing values\n",
    "\n",
    "def info_missing_table(df_pd):\n",
    "    \"\"\"Input pandas dataframe and Return columns with missing value and percentage\"\"\"\n",
    "    mis_val = df_pd.isnull().sum() #count total of null in each columns in dataframe\n",
    "    mis_val_percent = 100 * df_pd.isnull().sum() / len(df_pd) #count percentage of null in each columns\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)  #join to left (as column) between mis_val and mis_val_percent\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "    columns = {0 : 'Missing Values', 1 : '% of Total Values'}) #rename columns in table\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "    mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)         \n",
    "    print (\"Your selected dataframe has \" + str(df_pd.shape[1]) + \" columns.\\n\"    #.shape[1] : just view total columns in dataframe  \n",
    "    \"There are \" + str(mis_val_table_ren_columns.shape[0]) +              \n",
    "    \" columns that have missing values.\") #.shape[0] : just view total rows in dataframe\n",
    "    return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missings = info_missing_table(df_pd)\n",
    "missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so this function deals with the a spark dataframe directly to find more about the missing values\n",
    "\n",
    "def count_missings(spark_df):\n",
    "    null_counts = []        \n",
    "    for col in spark_df.dtypes:    \n",
    "        cname = col[0]     \n",
    "        ctype = col[1]      \n",
    "        nulls = spark_df.where( spark_df[cname].isNull()).count() #check count of null in column name\n",
    "        result = tuple([cname, nulls])  #new tuple, (column name, null count)\n",
    "        null_counts.append(result)      #put the new tuple in our result list\n",
    "    null_counts=[(x,y) for (x,y) in null_counts if y!=0]  #view just columns that have missing values\n",
    "    return null_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_counts = count_missings(new_df)\n",
    "miss_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we seperate missing columns in our new_df based on categorical and numerical types\n",
    "\n",
    "list_cols_miss=[x[0] for x in miss_counts]\n",
    "df_miss= new_df.select(*list_cols_miss)\n",
    "#categorical columns\n",
    "catcolums_miss=[item[0] for item in df_miss.dtypes if item[1].startswith('string')]  #will select name of column with string data type\n",
    "print(\"cateogrical columns_miss:\", catcolums_miss)\n",
    "\n",
    "### numerical columns\n",
    "numcolumns_miss = [item[0] for item in df_miss.dtypes if item[1].startswith('int') | item[1].startswith('double')] #will select name of column with integer or double data type\n",
    "# print(\"numerical columns_miss:\", numcolumns_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have seperated the columns based on categorical and numerical types, we will fill the missing categiracl \n",
    "# values with the most frequent category\n",
    "\n",
    "from pyspark.sql.functions import rank,sum,col\n",
    "df_Nomiss=new_df.na.drop()\n",
    "for x in catcolums_miss:\n",
    "    mode=df_Nomiss.groupBy(x).count().sort(col(\"count\").desc()).collect()[0][0] \n",
    "    print(x, mode) #print name of columns and it's most categories \n",
    "    new_df = new_df.na.fill({x:mode}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# and we fill the missing numerical values with the average of each column\n",
    "\n",
    "from pyspark.sql.functions import mean, round\n",
    "\n",
    "for i in numcolumns_miss:\n",
    "    meanvalue = new_df.select(round(mean(i))).collect()[0][0] \n",
    "    print(i, meanvalue) \n",
    "    new_df=new_df.na.fill({i:meanvalue}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "import re\n",
    "re_at = re.compile('@')\n",
    "def do_matchup(label):\n",
    "    if re_at.search(label):\n",
    "        return 0\n",
    "    else: \n",
    "        return 1\n",
    "\n",
    "do_matchup_udf = udf(lambda x: do_matchup(x), IntegerType())\n",
    "\n",
    "new_df = new_df.withColumn('home_game', do_matchup_udf(col('matchup')))\n",
    "val_df = val_df.withColumn('home_game', do_matchup_udf(col('matchup')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dealing with Imbalanced classes if necessary:\n",
    "### If the dataset is imbalanced, automatically add a new column to the dataset called \"weights\" and fill it with the ratio of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the new column weights and fill it with ratios\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "def weight_balance(labels):\n",
    "    return when(labels == 1, ratio).otherwise(1*(1-ratio))\n",
    "\n",
    "if imbalanced:\n",
    "    new_df = new_df.withColumn('weights', weight_balance(col('label')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.drop('matchup')\n",
    "val_df = val_df.drop('matchup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's count if we have still any missing values in our dataset:\n",
    "\n",
    "miss_counts2 = count_missings(new_df)\n",
    "miss_counts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and have another look at the data after filling the missing values and adding the weight column\n",
    "pd.DataFrame(new_df.take(10), columns= new_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's see how many categorical and numerical features we have:\n",
    "\n",
    "cat_cols = [item[0] for item in new_df.dtypes if item[1].startswith('string')] \n",
    "print(str(len(cat_cols)) + '  categorical features')\n",
    "\n",
    "#num_cols = [item[0] for item in new_df.dtypes if item[1].startswith('int') | item[1].startswith('double')][1:]\n",
    "\n",
    "#don't include 'label' column as a numeric feature.  That would be cheating and result in a perfect model\n",
    "num_cols = []\n",
    "for item in new_df.dtypes:\n",
    "    if item[0] == 'label':\n",
    "        print('found label')\n",
    "    else:\n",
    "        if item[1].startswith('int') | item[1].startswith('double'):\n",
    "            num_cols += [item[0]]\n",
    "            \n",
    "print(str(len(num_cols)) + '  numerical features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the OneHotEncoderEstimator from MLlib in spark to convert aech v=categorical feature into one-hot vectors\n",
    "# next, we use VectorAssembler to combine the resulted one-hot ector and the rest of numerical features into a \n",
    "# single vector column. we append every step of the process in a stages array\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "\n",
    "stages = []\n",
    "for categoricalCol in cat_cols:\n",
    "    print(categoricalCol)\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index').setHandleInvalid(\"keep\")\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "assemblerInputs = [c + \"classVec\" for c in cat_cols] + num_cols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "numeric_data = new_df.select('loc_x','loc_y').toPandas()\n",
    "#numeric_data = new_df.select(num_cols).toPandas()\n",
    "\n",
    "axs = scatter_matrix(numeric_data, figsize=(8, 8));\n",
    "n = len(numeric_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a pipeline to apply all the stages of tranformation to the data\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "cols = new_df.columns\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(new_df)\n",
    "new_df = pipelineModel.transform(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedCols = ['features']+cols\n",
    "new_df = new_df.select(selectedCols)\n",
    "pd.DataFrame(new_df.take(5), columns=new_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split the data into trainign and testin sets\n",
    "\n",
    "train, test = new_df.randomSplit([0.90, 0.10], seed = 53)\n",
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we check how LogisticRegression perform \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=15)\n",
    "LR_model = LR.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the ROC Curve\n",
    "\n",
    "trainingSummary = LR_model.summary\n",
    "\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "print('Training set ROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "beta = np.sort(LR_model.coefficients)\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predictions_LR = LR_model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test_SET Area Under ROC: \" + str(evaluator.evaluate(predictions_LR, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = trainingSummary.pr.toPandas()\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we checkout gradient boosting trees\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(maxIter=15)\n",
    "GBT_Model = gbt.fit(train)\n",
    "predictions = GBT_Model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test_SET Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the data thru a Cross Validator to find the best gbt model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "    paramGrid = (ParamGridBuilder()\n",
    "                 .addGrid(gbt.maxDepth, [2, 4, 6])\n",
    "                 .addGrid(gbt.maxBins, [20, 30])\n",
    "                 .addGrid(gbt.maxIter, [10, 15])\n",
    "                 .build())\n",
    "\n",
    "    cv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "    # Run cross validations.\n",
    "    cvModel = cv.fit(train)\n",
    "    predictions = cvModel.transform(test)\n",
    "    evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peek at test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.select('shot_type','shot_distance','label','prediction').limit(100).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peek at the validation set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send the validation set (set used for the competition) thru the data transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_val_df = pipelineModel.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_val_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_val_df = transform_val_df.drop('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Predictions based on the GBT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = GBT_Model.transform(transform_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a peek at some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions.select('action_type','shot_zone_area','shot_distance','opponent','home_game','prediction').limit(100).toPandas()\n",
    "#val_predictions.select('features').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the best model if using Cross-Validation or save the only model if using stright Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cvModel.bestModel.save('kobe.gt')\n",
    "    cvModel.save('kobe.gbt.model')\n",
    "except NameError:\n",
    "    GBT_Model.save('kobe.gbt.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the predicitons in the form (shot_id, prediciton) in a single .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions.select('shot_id','prediction').coalesce(1).write.csv('kobe.predictions.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions.select('shot_id','prediction').limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
